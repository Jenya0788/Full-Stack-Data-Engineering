{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66586eb7-c25b-4cce-80b5-2b3f5ff048d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Scraping page 1: https://old.reddit.com\n",
      "üîÑ Scraping page 2: https://old.reddit.com/?count=25&after=t3_1lf1gg9\n",
      "üîÑ Scraping page 3: https://old.reddit.com/?count=50&after=t3_1lf7mus\n",
      "üîÑ Scraping page 4: https://old.reddit.com/?count=75&after=t3_1lf8eyt\n",
      "üîÑ Scraping page 5: https://old.reddit.com/?count=100&after=t3_1lfbfpa\n",
      "‚úÖ Excel file with real hyperlinks saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74137c0-ca3a-481e-aca9-c21678e18c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conn_web(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.content.decode('utf-8')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error connecting to {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8cc1d8d-efe3-403f-b75a-84b5a97f5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xpath_anchoring(content, data_list):\n",
    "    try:\n",
    "        html = etree.HTML(content)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing HTML: {e}\")\n",
    "        return data_list, None\n",
    "\n",
    "    posts = html.xpath('//p[@class=\"title\"]/a[@data-event-action=\"title\"]')\n",
    "\n",
    "    for post in posts:\n",
    "        text = post.xpath('./text()')\n",
    "        text = text[0].strip() if text else \"\"\n",
    "\n",
    "        href = post.xpath('./@href')\n",
    "        href = href[0].strip() if href else \"\"\n",
    "\n",
    "        if href.startswith('/'):\n",
    "            href = \"https://old.reddit.com\" + href\n",
    "\n",
    "        data_list.append({\n",
    "            \"title\": text,\n",
    "            \"url\": href\n",
    "        })\n",
    "\n",
    "    next_url = html.xpath('//span[@class=\"next-button\"]/a/@href')\n",
    "    next_url = next_url[0] if next_url else None\n",
    "\n",
    "    return data_list, next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d447840-f063-4e49-aa2f-0c76e9577527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_scraper_loop(start_url, page_num):\n",
    "    data_list = []\n",
    "    current_url = start_url\n",
    "\n",
    "    for i in range(page_num):\n",
    "        print(f\"üîÑ Scraping page {i+1}: {current_url}\")\n",
    "        content = conn_web(current_url)\n",
    "        if content is None:\n",
    "            print(f\"‚ö†Ô∏è Skipping page {i+1} due to connection error.\")\n",
    "            continue\n",
    "\n",
    "        data_list, current_url = xpath_anchoring(content, data_list)\n",
    "\n",
    "        if not current_url:\n",
    "            print(\"üöß No next page found. Stopping early.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a832607c-16dc-4adb-9e90-f5468fee9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel_with_real_hyperlinks(df, filename):\n",
    "    with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, sheet_name='RedditPosts', index=False, startrow=1, header=False)\n",
    "\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['RedditPosts']\n",
    "        header_format = workbook.add_format({'bold': True, 'bg_color': '#DDEBF7'})\n",
    "        link_format = workbook.add_format({'font_color': 'blue', 'underline': 1})\n",
    "\n",
    "        # Write headers manually\n",
    "        for col_num, value in enumerate(df.columns):\n",
    "            worksheet.write(0, col_num, value, header_format)\n",
    "\n",
    "        # Write actual hyperlinks\n",
    "        for row in range(len(df)):\n",
    "            title = df.at[row, \"title\"]\n",
    "            url = df.at[row, \"url\"]\n",
    "\n",
    "            if isinstance(url, str) and url.startswith(\"http\"):\n",
    "                worksheet.write_url(row + 1, 1, url, link_format, string=title)\n",
    "            else:\n",
    "                worksheet.write(row + 1, 1, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3dc5421-4f18-4b7b-b9dd-3147547d5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_url = \"https://old.reddit.com\"\n",
    "    page_num = 5\n",
    "\n",
    "    scraped_data = reddit_scraper_loop(start_url, page_num)\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            save_to_excel_with_real_hyperlinks(df, \"reddit_output.xlsx\")\n",
    "            print(\"‚úÖ Excel file with real hyperlinks saved successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error writing Excel: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data scraped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ac646-9df5-4725-82ba-887dad77c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
